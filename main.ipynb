{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write implementation on PyTorch for TransE model (you can use TorchGeometric or DGL library for working with graphs) and train your model on WordNet18RR dataset (you can use loaded dataset from any graph library).\n",
    "\n",
    "As a result, you must provide a link to github (or gitlab) with all the source code.\n",
    "The readability of the code, the presence of comments, type annotations, and the quality of the code as a whole will be taken into account when checking the test case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pkg_resources\\__init__.py:122: PkgResourcesDeprecationWarning: otobuf is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union, Callable, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from torch_geometric.datasets import WordNet18RR\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download wordnet dataset, we'll be using the processed file data.pt\n",
    "dataset = WordNet18RR('./WordNet18RR/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_norm = lambda x, y: torch.linalg.norm((x-y), ord=1)\n",
    "l2_norm = lambda x, y: torch.linalg.norm((x-y), ord=2)\n",
    "\n",
    "def normalize(x: Union[torch.Tensor, np.ndarray], axis: int=1):\n",
    "    \"\"\" Normalize the matrix x along the specified axis\n",
    "\n",
    "    Args:\n",
    "        x (Union[torch.Tensor, np.ndarray]): _description_\n",
    "        axis (int, optional): 0 = columns, 1 = rows. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        (Union[torch.Tensor, np.ndarray]): returns a matrix with the same dtype as the input\n",
    "    \"\"\"\n",
    "    return_tensor = False\n",
    "\n",
    "    if x.dtype == torch.Tensor:\n",
    "        x = x.numpy()\n",
    "        return_tensor = True\n",
    "    \n",
    "    x = np.apply_along_axis(func1d=lambda x: x / np.linalg.norm(x), arr=x, axis=axis)\n",
    "\n",
    "    if return_tensor:\n",
    "        return torch.from_numpy(x)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class Edge():\n",
    "    def __init__(self, u, v, label) -> None:\n",
    "        self.u = u\n",
    "        self.v = v\n",
    "        self.label = label\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.u} {self.label} {self.v}\"\n",
    "\n",
    "def load_edge_list_from_file(path: str, header: bool=False):\n",
    "    edge_list = list()\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "\n",
    "        if header:\n",
    "            next(tsv_reader)\n",
    "\n",
    "        for row in tsv_reader:\n",
    "            u, label, v = row\n",
    "            edge_list.append(Edge(u=u, v=v, label=label))\n",
    "    \n",
    "    return edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordNetEdgeDataset(Dataset):\n",
    "    def __init__(self, path: str=\"WordNet18RR/processed/data.pt\", split: str=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        data = torch.load(path)[0]\n",
    "        mask_dict = {\"train\": data.train_mask, \"test\": data.test_mask, \"val\": data.val_mask}\n",
    "        mask = mask_dict[split]\n",
    "        self.edge_list = data.edge_index.T[mask, :]\n",
    "        self.edge_labels = data.edge_type[mask]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.edge_list.shape[0]\n",
    "\n",
    "    def __getitem__(self, index) -> int:\n",
    "        return self.edge_list[index,:], self.edge_labels[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordNetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str=\"WordNet18RR/processed/data.pt\", batch_size=32) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_entities = 40943\n",
    "        self.num_relations = 11\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = WordNetEdgeDataset(split=\"train\", path=self.data_dir)\n",
    "            self.val_dataset = WordNetEdgeDataset(split=\"val\", path=self.data_dir)\n",
    "        \n",
    "        if stage == \"test\":\n",
    "            self.test_dataset = WordNetEdgeDataset(split=\"test\", path=self.data_dir)\n",
    "            self.test_loader = DataLoader(self.test_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_corrupted_triplet(x: torch.Tensor, n: int):\n",
    "    \"\"\" sample either the head or tail of x from range(n)\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): a pair of ints\n",
    "        n (int): number of entities to sample from\n",
    "    \"\"\"\n",
    "    # sample corrupted triplet\n",
    "    idx = int(np.random.rand(1) < 0.5)  # pick either head or tail\n",
    "    s = x.detach().clone()\n",
    "    while True:\n",
    "        s[idx] = np.random.choice(range(dm.num_entities))   # resample either head or tail\n",
    "\n",
    "        # make sure the triples are different\n",
    "        if s[idx] != x[idx]:\n",
    "            break\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(pl.LightningModule):\n",
    "    def __init__(self, margin: int=1, emb_dim: int=50, learning_rate=0.01,\n",
    "                 distance_func: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]=l1_norm) -> None:\n",
    "        \"\"\" Instatiate the entity and relation matrix of the TransE model\n",
    "            https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data\n",
    "\n",
    "        Args:\n",
    "            n_entities (int): _description_\n",
    "            n_relations (int): _description_\n",
    "            margin (int, optional): _description_. Defaults to 1.\n",
    "            emb_dim (int, optional): _description_. Defaults to 50.\n",
    "            distance_func (Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional): _description_. Defaults to l1_norm.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.emb_dim = emb_dim\n",
    "        self.distance_func = distance_func\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # dataset specific values\n",
    "        self.num_entities = 40943\n",
    "        self.num_relations = 11\n",
    "\n",
    "        # initialize embeddings\n",
    "        self.entity_mat = nn.Embedding(self.num_entities, emb_dim)\n",
    "        self.relation_mat = nn.Embedding(self.num_relations, emb_dim)\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            # initialize with random uniform\n",
    "            val = 6/np.sqrt(emb_dim)\n",
    "            self.entity_mat.weight.uniform_(-val, val)\n",
    "            self.relation_mat.weight.uniform_(-val, val)\n",
    "\n",
    "            # normalize each embedding vector\n",
    "            self.entity_mat.weight.copy_(nn.functional.normalize(self.entity_mat.weight, p=2, dim=1))\n",
    "            self.relation_mat.weight.copy_(nn.functional.normalize(self.relation_mat.weight, p=2, dim=1))        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\" batch is the list of ids of edge within the batch \"\"\"\n",
    "        edge_list, labels = batch\n",
    "        \n",
    "        loss = torch.zeros(1)\n",
    "\n",
    "        for i in range(edge_list.shape[0]):\n",
    "            x = edge_list[i]\n",
    "            x_corrupted = sample_corrupted_triplet(x, n=self.num_entities)\n",
    "\n",
    "            # take embedding values for entities and relation\n",
    "            h1, t1, = self.entity_mat.weight[x,:]\n",
    "            h2, t2 = self.entity_mat.weight[x_corrupted,:]\n",
    "            l = self.relation_mat.weight[labels[i]]\n",
    "\n",
    "            # compute the loss value\n",
    "            val = self.margin + torch.norm(h1 + l - t1) - torch.norm(h2 + l - t2)\n",
    "\n",
    "            nn.functional.relu(val, inplace=True) # take positive part\n",
    "            \n",
    "            loss += val\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_training_epoch_end(self, epoch_idx: int):\n",
    "        with torch.nograd():\n",
    "             # keep embeddings normalized\n",
    "            self.entity_mat.weight.copy_(nn.functional.normalize(self.entity_mat.weight, p=2, dim=1))\n",
    "            self.relation_mat.weight.copy_(nn.functional.normalize(self.relation_mat.weight, p=2, dim=1))       \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Uni\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:106: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "\n",
      "  | Name         | Type      | Params\n",
      "-------------------------------------------\n",
      "0 | entity_mat   | Embedding | 204 K \n",
      "1 | relation_mat | Embedding | 55    \n",
      "-------------------------------------------\n",
      "204 K     Trainable params\n",
      "0         Non-trainable params\n",
      "204 K     Total params\n",
      "0.819     Total estimated model params size (MB)\n",
      "C:\\Users\\Uni\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  64%|██████▍   | 1750/2714 [10:20<05:41,  2.82it/s, loss=26.8, v_num=3]\n",
      "Epoch 0:  43%|████▎     | 1160/2714 [02:41<03:35,  7.20it/s, loss=29.3, v_num=4]"
     ]
    }
   ],
   "source": [
    "model = TransE(emb_dim=5)\n",
    "dm = WordNetDataModule()\n",
    "trainer = pl.Trainer(max_epochs=1)\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
