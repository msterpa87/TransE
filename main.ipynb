{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write implementation on PyTorch for TransE model (you can use TorchGeometric or DGL library for working with graphs) and train your model on WordNet18RR dataset (you can use loaded dataset from any graph library).\n",
    "\n",
    "As a result, you must provide a link to github (or gitlab) with all the source code.\n",
    "The readability of the code, the presence of comments, type annotations, and the quality of the code as a whole will be taken into account when checking the test case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union, Callable, Optional\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from torch_geometric.datasets import WordNet18RR\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download wordnet dataset, we'll be using the processed file data.pt\n",
    "dataset = WordNet18RR('./WordNet18RR/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Edge():\n",
    "    def __init__(self, u, v, label) -> None:\n",
    "        self.u = u\n",
    "        self.v = v\n",
    "        self.label = label\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.u} {self.label} {self.v}\"\n",
    "\n",
    "def load_edge_list_from_file(path: str, header: bool=False):\n",
    "    edge_list = list()\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "\n",
    "        if header:\n",
    "            next(tsv_reader)\n",
    "\n",
    "        for row in tsv_reader:\n",
    "            u, label, v = row\n",
    "            edge_list.append(Edge(u=u, v=v, label=label))\n",
    "    \n",
    "    return edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordNetEdgeDataset(Dataset):\n",
    "    def __init__(self, path: str=\"WordNet18RR/processed/data.pt\", split: str=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        data = torch.load(path)[0]\n",
    "        mask_dict = {\"train\": data.train_mask, \"test\": data.test_mask, \"val\": data.val_mask}\n",
    "        mask = mask_dict[split]\n",
    "        self.edge_list = data.edge_index.T[mask, :]\n",
    "        self.edge_labels = data.edge_type[mask]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.edge_list.shape[0]\n",
    "\n",
    "    def __getitem__(self, index) -> int:\n",
    "        return self.edge_list[index,:], self.edge_labels[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordNetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str=\"WordNet18RR/processed/data.pt\", batch_size=32, num_workers=6) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_entities = 40943\n",
    "        self.num_relations = 11\n",
    "        self.num_workers = num_workers\n",
    "        self.loader_params = {\n",
    "            'batch_size': batch_size,\n",
    "            'pin_memory': True,\n",
    "            'shuffle': True\n",
    "        }\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = WordNetEdgeDataset(split=\"train\", path=self.data_dir)\n",
    "            self.val_dataset = WordNetEdgeDataset(split=\"val\", path=self.data_dir)\n",
    "        \n",
    "        if stage == \"test\":\n",
    "            self.test_dataset = WordNetEdgeDataset(split=\"test\", path=self.data_dir)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, **self.loader_params)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, **self.loader_params)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, **self.loader_params)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(pl.LightningModule):\n",
    "    def __init__(self, margin: int=1, emb_dim: int=50, learning_rate=0.01) -> None:\n",
    "        \"\"\" Instatiate the entity and relation matrix of the TransE model\n",
    "            https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data\n",
    "\n",
    "        Args:\n",
    "            n_entities (int): _description_\n",
    "            n_relations (int): _description_\n",
    "            margin (int, optional): _description_. Defaults to 1.\n",
    "            emb_dim (int, optional): _description_. Defaults to 50.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.emb_dim = emb_dim\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # dataset specific values\n",
    "        self.num_entities = 40943\n",
    "        self.num_relations = 11\n",
    "\n",
    "        # initialize embeddings\n",
    "        self.entity_mat = nn.Embedding(self.num_entities, emb_dim).to(self.device)\n",
    "        self.relation_mat = nn.Embedding(self.num_relations, emb_dim).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # initialize with random uniform\n",
    "            val = 6/np.sqrt(emb_dim)\n",
    "            self.entity_mat.weight.uniform_(-val, val)\n",
    "            self.relation_mat.weight.uniform_(-val, val)\n",
    "\n",
    "            # normalize entity embeddings\n",
    "            self.entity_mat.weight.copy_(nn.functional.normalize(self.entity_mat.weight, p=2, dim=1))\n",
    "\n",
    "    def corrupt_edge_list(self, edge_list: torch.Tensor):\n",
    "        \"\"\" sample either the head or tail of x from range(n) \"\"\"\n",
    "        idxs = (np.random.rand(edge_list.shape[0]) < 0.5).astype(int)  # pick either head or tail\n",
    "        s = edge_list.detach().clone()\n",
    "\n",
    "        # sample random entity replacements\n",
    "        vals = np.random.choice(range(self.num_entities), size=edge_list.shape[0])\n",
    "\n",
    "        for i,idx in enumerate(idxs):\n",
    "            s[i,idx] = vals[i]\n",
    "        \n",
    "        return s\n",
    "\n",
    "    def embedding_loss(self, batch):\n",
    "        edge_list, labels = batch\n",
    "        \n",
    "        loss = torch.zeros(1).to(self.device)\n",
    "\n",
    "        edge_list_cor = self.corrupt_edge_list(edge_list)\n",
    "        \n",
    "        # take embedding values for entities and relations\n",
    "        e1 = self.entity_mat.weight[edge_list]\n",
    "        e2 = self.entity_mat.weight[edge_list_cor]\n",
    "        l = self.relation_mat.weight[labels]\n",
    "\n",
    "        # compute the loss value\n",
    "        n1 = torch.norm(e1[:,0,:] + l - e1[:,1,:], dim=1)\n",
    "        n2 = torch.norm(e2[:,0,:] + l - e2[:,1,:], dim=1)\n",
    "        loss = (self.margin + n1 - n2)\n",
    "        loss = torch.clip(loss, min=0).sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluation_protocol(self, batch):\n",
    "        edge_list, labels = batch\n",
    "        rankings_list = list()\n",
    "        hits_at_10_list = list()\n",
    "\n",
    "        for i in range(edge_list.shape[0]):\n",
    "            # take a single test triplet\n",
    "            test_triplet = edge_list[i].T\n",
    "\n",
    "            n = self.num_entities\n",
    "\n",
    "            # replicate triplet for num_entities time for corruption\n",
    "            entities = torch.tensor(list(range(self.num_entities)))\n",
    "            x_cor = test_triplet.repeat(n, 1)\n",
    "\n",
    "            # relation embeddings\n",
    "            l = self.relation_mat[labels[i].repeat(n)]\n",
    "\n",
    "            # compute ranking and hits@10 by corrupting both head and tail\n",
    "            for pos in [0,1]:\n",
    "                # replace triplet head with each possible entities\n",
    "                x_cor[:, pos] = entities\n",
    "                val = test_triplet[pos]\n",
    "\n",
    "                # get entity matrix for all possible pairings\n",
    "                e1 = self.entity_mat[x_cor]\n",
    "\n",
    "                # compute distance between head + label and tail\n",
    "                dissimilarities = torch.norm(e1[:,0,:] + l - e1[:,1,:], dim=1)\n",
    "\n",
    "                # rank distances in ascending order\n",
    "                ranking = torch.argsort(dissimilarities)\n",
    "\n",
    "                # find position of true triplet within ranking and if is <10\n",
    "                test_pos = torch.where(ranking == val)[0].item()\n",
    "                is_among_10 = (ranking[:10] == val).sum().item()\n",
    "\n",
    "                # save current rank to later compute test results\n",
    "                rankings_list.append(test_pos)\n",
    "                hits_at_10_list.append(is_among_10)\n",
    "    \n",
    "        metrics = {\n",
    "            \"mean_rank\": np.mean(rankings_list).astype(int),\n",
    "            \"hits_at_10\": np.sum(hits_at_10_list)\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.embedding_loss(batch)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.embedding_loss(batch)\n",
    "        metrics = self.evaluation_protocol(batch)\n",
    "        metrics[\"val_loss\": loss]\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.embedding_loss(batch)\n",
    "        metrics = self.evaluation_protocol(batch)\n",
    "        metrics[\"test_loss\": loss]\n",
    "        return metrics\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        with torch.no_grad():\n",
    "             # keep entities embeddings normalized\n",
    "            self.entity_mat.weight.copy_(nn.functional.normalize(self.entity_mat.weight, p=2, dim=1))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type      | Params\n",
      "-------------------------------------------\n",
      "0 | entity_mat   | Embedding | 204 K \n",
      "1 | relation_mat | Embedding | 55    \n",
      "-------------------------------------------\n",
      "204 K     Trainable params\n",
      "0         Non-trainable params\n",
      "204 K     Total params\n",
      "0.819     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Uni\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Uni\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1405/1405 [00:19<00:00, 70.37it/s, loss=52.1, v_num=64, val_loss=53.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1405/1405 [00:19<00:00, 70.32it/s, loss=52.1, v_num=64, val_loss=53.40]\n"
     ]
    }
   ],
   "source": [
    "model = TransE(emb_dim=5)\n",
    "dm = WordNetDataModule(batch_size=64)\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"TransE\")\n",
    "trainer = pl.Trainer(max_epochs=1, accelerator='gpu')\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9371, 0.6720, 1.9015,  ..., 4.8295, 4.3622, 3.9676],\n",
       "       grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(model.entity_mat.weight, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_rank = 16216\n",
      "hits@10 =  0\n"
     ]
    }
   ],
   "source": [
    "entity_mat = model.entity_mat.weight\n",
    "relation_mat = model.relation_mat.weight\n",
    "\n",
    "rankings_list = list()\n",
    "hit_at_10_list = list()\n",
    "\n",
    "for i in range(batch.shape[0]):\n",
    "    # take a single test triplet\n",
    "    x = batch[i].T\n",
    "    head, tail = x\n",
    "\n",
    "    n = model.num_entities\n",
    "\n",
    "    # all possible entities list\n",
    "    entities = torch.tensor(list(range(model.num_entities)))\n",
    "    x_cor = x.repeat(n, 1)\n",
    "\n",
    "    # relation embeddings\n",
    "    l = relation_mat[labels[i].repeat(n)]\n",
    "\n",
    "    # compute ranking and hits@10 by corrupting both head and tail\n",
    "    for pos in [0,1]:\n",
    "        # replace triplet head with each possible entities\n",
    "        x_cor[:, pos] = entities\n",
    "        val = x[pos]\n",
    "\n",
    "        # get entity matrix for all possible pairings\n",
    "        e1 = entity_mat[x_cor]\n",
    "\n",
    "        # compute distance between head + label and tail\n",
    "        dissimilarities = torch.norm(e1[:,0,:] + l - e1[:,1,:], dim=1)\n",
    "\n",
    "        # rank distances in ascending order\n",
    "        ranking = torch.argsort(dissimilarities)\n",
    "\n",
    "        # find position of true triplet within ranking and if is <10\n",
    "        test_pos = torch.where(ranking == val)[0].item()\n",
    "        is_among_10 = (ranking[:10] == val).sum().item()\n",
    "\n",
    "        # save current rank to later compute test results\n",
    "        rankings_list.append(test_pos)\n",
    "        hit_at_10_list.append(is_among_10)\n",
    "\n",
    "print(\"mean_rank =\", np.mean(rankings_list).astype(int))\n",
    "print(\"hits@10 = \", np.sum(hit_at_10_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18045)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = entity_mat[batch]\n",
    "l = relation_mat[labels]\n",
    "batch_cor = model.corrupt_batch(next(iter(dm.train_dataloader())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(59.9413, grad_fn=<ClampBackward1>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margin = 1\n",
    "\n",
    "# take embedding values for entities and relations\n",
    "e1 = entity_mat[batch]\n",
    "e2 = entity_mat[batch_cor]\n",
    "l = relation_mat[labels]\n",
    "\n",
    "# compute the loss value\n",
    "n1 = torch.norm(e1[:,0,:] + l - e1[:,1,:], dim=1)\n",
    "n2 = torch.norm(e2[:,0,:] + l - e2[:,1,:], dim=1)\n",
    "\n",
    "loss = (margin + n1 - n2).sum()\n",
    "loss = torch.clip(loss, min=0)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(113.5347, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(entity_mat[batch[:,0]] + relation_mat[labels] - entity_mat[batch[:,1]], dim=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
