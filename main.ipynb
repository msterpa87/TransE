{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translational Embeddings for Modeling Multi-relational Data\n",
    "In this notebook we implement the [TransE](https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data) model and test its performance for the task of link prediction on the WordNet18RR dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from contextlib import suppress\n",
    "from typing import List, Union, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.datasets import WordNet18RR, WordNet18\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# needed \n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# check if CUDA device is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both versions of the WordNet18 dataset have been imported through the PyTorch Geometric library. We highlight that training/testing on the original WN18 was done solely to compare the implemented model best performance against the results from **Borders et al.** (table 3 page 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordNet18RR()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download wordnet dataset\n",
    "WordNet18('./WordNet18/')\n",
    "WordNet18RR('./WordNet18RR/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset and DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a custom dataset that reads from the raw directory of the WordNet18 downloaded in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge():\n",
    "    def __init__(self, head, tail, rel) -> None:\n",
    "        self.head = head\n",
    "        self.tail = tail\n",
    "        self.rel = rel\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.head} {self.rel} {self.tail}\"\n",
    "\n",
    "def process_lines(lines: List[str], delim: str='\\t'):\n",
    "    \"\"\" cleans up the input set of strings \"\"\"\n",
    "    return list(map(lambda s: s.strip('\\n').split(delim), lines))\n",
    "\n",
    "def load_edges_from_file(path: str, is_wn18: bool=True):\n",
    "    \"\"\" read edges from the text file in raw, considering the different\n",
    "        formats of RR (head, rel, tail) and original version (head, tail, rel) \"\"\"\n",
    "    edge_list = list()\n",
    "\n",
    "    lines = open(path).readlines()\n",
    "\n",
    "    # WN18 contains a header line and has a format (head, tail, rel)\n",
    "    if is_wn18:\n",
    "        lines = lines[1:]\n",
    "        delim = ' '\n",
    "    else:\n",
    "        delim = '\\t'\n",
    "\n",
    "    lines = process_lines(lines, delim=delim)\n",
    "\n",
    "    # the two WN version have a different format to represent edges/relation\n",
    "    if is_wn18:\n",
    "        edge_list = [Edge(head=head, tail=tail, rel=rel) for head, tail, rel in lines]\n",
    "    else:\n",
    "        edge_list = [Edge(head=head, tail=tail, rel=rel) for head, rel, tail in lines]\n",
    "    \n",
    "    return edge_list\n",
    "\n",
    "def load_ids_dict(path: str) -> Union[dict, dict]:\n",
    "    \"\"\" reads and return the dictionaries entity->id and relation->id \n",
    "        from the specified location \"\"\"\n",
    "    \n",
    "    assert(os.path.exists(path))\n",
    "\n",
    "    entity2id = process_lines(open(os.path.join(path, \"entity2id.txt\")))\n",
    "    relation2id = process_lines(open(os.path.join(path, \"relation2id.txt\")))\n",
    "\n",
    "    entity2id = dict([(x[0], int(x[1])) for x in entity2id])\n",
    "    relation2id = dict([(x[0], int(x[1])) for x in relation2id])\n",
    "\n",
    "    return entity2id, relation2id\n",
    "\n",
    "def create_id_mappings(dataset_str: str=\"WordNet18RR\") -> None:\n",
    "    \"\"\" creates the mapping ids inside the raw directory of the \n",
    "        specified version of WordNet18 \"\"\"\n",
    "\n",
    "    assert(dataset_str in [\"WordNet18\", \"WordNet18RR\"])\n",
    "\n",
    "    is_wn18 = dataset_str == \"WordNet18\"\n",
    "    path = f\"./{dataset_str}/raw/\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory {path} does not exist\")\n",
    "        return\n",
    "    \n",
    "    # read edge_list from the raw text files\n",
    "    train_edge_list = load_edges_from_file(os.path.join(path, \"train.txt\"), is_wn18=is_wn18)\n",
    "    val_edge_list = load_edges_from_file(os.path.join(path, \"valid.txt\"), is_wn18=is_wn18)\n",
    "    test_edge_list = load_edges_from_file(os.path.join(path, \"test.txt\"), is_wn18=is_wn18)\n",
    "\n",
    "    entity_list = list()\n",
    "    relation_list = list()\n",
    "\n",
    "    # assign unique id to each entity/relation\n",
    "    for edge_list in [train_edge_list, val_edge_list, test_edge_list]:\n",
    "        entity_list += [x.head for x in edge_list] + [x.tail for x in edge_list]\n",
    "        relation_list += [x.rel for x in edge_list]\n",
    "\n",
    "    entity_list = sorted(list(set(entity_list)))\n",
    "    entity2id = dict(zip(entity_list, range(len(entity_list))))\n",
    "\n",
    "    relation_list = sorted(list(set(relation_list)))\n",
    "    relation2id = dict(zip(relation_list, range(len(relation_list))))\n",
    "\n",
    "    # save the generated mappings into the raw directory\n",
    "    with open(os.path.join(path, \"entity2id.txt\"), \"w\") as f:\n",
    "        f.writelines([f\"{x}\\t{y}\\n\" for x,y in entity2id.items()])\n",
    "\n",
    "    with open(os.path.join(path, \"relation2id.txt\"), \"w\") as f:\n",
    "        f.writelines([f\"{x}\\t{y}\\n\" for x,y in relation2id.items()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the mappings we simply call the *create_id_mappings()* function specifying the WN version we want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_id_mappings(\"WordNet18RR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a PyTorch dataset and a Data Module that can be handled by PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordNetDataset(Dataset):\n",
    "    def __init__(self, dataset: str=\"WordNet18RR\", split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        self.path = f\"./{dataset}/raw\"\n",
    "\n",
    "        if split == 'val':\n",
    "            split = 'valid'\n",
    "        self.split = split\n",
    "\n",
    "        is_wn18 = dataset == \"WordNet18\"\n",
    "\n",
    "        edge_list = load_edges_from_file(os.path.join(self.path, f\"{self.split}.txt\"), is_wn18=is_wn18)\n",
    "        entity2id, relation2id = load_ids_dict(path=self.path)\n",
    "\n",
    "        self.edge_list = torch.tensor([(entity2id[e.head], entity2id[e.tail]) for e in edge_list])\n",
    "        self.relation_list = torch.tensor([relation2id[e.rel] for e in edge_list])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.edge_list.shape[0]\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[int,int]:\n",
    "        return self.edge_list[index], self.relation_list[index]\n",
    "\n",
    "class WordNetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dataset: str=\"WordNet18RR\", batch_size=32) -> None:\n",
    "        super().__init__()\n",
    "        self.path = f\"./{dataset}/raw\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_entities = 40943\n",
    "\n",
    "        if dataset == 'WordNet18RR':\n",
    "            self.num_relations = 11\n",
    "        else:\n",
    "            self.num_relations = 18\n",
    "        \n",
    "        self.params = {\"pin_memory\": True, \"batch_size\": batch_size}\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = WordNetDataset(dataset=self.dataset, split=\"train\")\n",
    "            self.val_dataset = WordNetDataset(dataset=self.dataset, split=\"valid\")\n",
    "        \n",
    "        if stage == \"predict\":\n",
    "            self.test_dataset = WordNetDataset(dataset=self.dataset, split=\"test\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, shuffle=True, **self.params)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, shuffle=False, **self.params)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, shuffle=False, **self.params)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Model\n",
    "\n",
    "We start by defining a custom TransE model that implements the margin ranking loss as well as the corruption procedure from the paper.\n",
    "\n",
    "For clarity the *double_corrupt_edge_list()* was tested but ultimately dropped in the model analysis as it implements a corruption procedure that works by creating 2 copies of a batch where in each first the heads and then the tails are replaced by a random entity. This is due to some confusion raised from the formulat (2) in section 2 page 3, where the set $$S'$$ seems to include both version of the triplet (head or tail replaced).\n",
    "\n",
    "But then a few lines later the authors claim *\"Then, a smallset of triplets is sampled from the training set, and will serve as the training triplets of the minibatch. For each such triplet, we then sample a single corrupted triplet.\"* which we interpreted as building a single corrupted list of triplets for a batch where for each triplet we replace the head or the tail.\n",
    "\n",
    "We highlight how training using *double_corrupt_edge_list()* doesn't result in any noticeble change in performance, which we kept just for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(pl.LightningModule):\n",
    "    def __init__(self, margin: int=1, emb_dim: int=20, learning_rate=0.01, p_norm=1, dataset=\"WordNet18RR\") -> None:\n",
    "        \"\"\" Instatiate the entity and relation matrix of the TransE model\n",
    "            https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data\n",
    "\n",
    "        Args:\n",
    "            n_entities (int): _description_\n",
    "            n_relations (int): _description_\n",
    "            margin (int, optional): _description_. Defaults to 1.\n",
    "            emb_dim (int, optional): _description_. Defaults to 50.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.emb_dim = emb_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.p_norm = p_norm\n",
    "\n",
    "        # dataset specific values\n",
    "        self.num_entities = 40943\n",
    "\n",
    "        if dataset == \"WordNet18\":\n",
    "            self.num_relations = 18\n",
    "        else:\n",
    "            self.num_relations = 11\n",
    "\n",
    "        # initialize embeddings\n",
    "        self.entity_mat = nn.Embedding(self.num_entities, emb_dim).to(self.device)\n",
    "        self.relation_mat = nn.Embedding(self.num_relations, emb_dim).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # initialize with random uniform\n",
    "            val = 6/np.sqrt(emb_dim)\n",
    "            self.entity_mat.weight.uniform_(-val, val)\n",
    "            self.relation_mat.weight.uniform_(-val, val)\n",
    "\n",
    "            # normalize entity and relation embeddings\n",
    "            self.entity_mat.weight.copy_(F.normalize(self.entity_mat.weight, p=self.p_norm, dim=-1))\n",
    "            self.relation_mat.weight.copy_(F.normalize(self.relation_mat.weight, p=self.p_norm, dim=-1))\n",
    "\n",
    "    def double_corrupt_edge_list(self, edge_list: torch.Tensor) -> Union[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" given a list of triplets return two corrupted lists, the first randomly replacing\n",
    "            head entities, the second randomly replacing tail entities\n",
    "            NOTE: this function has not been used but was part of the experiments \"\"\"\n",
    "        n = edge_list.shape[0]\n",
    "        entity_list = range(self.num_entities)\n",
    "\n",
    "        # sample random entity replacements\n",
    "        r1 = np.random.choice(entity_list, size=n)\n",
    "        r2 = np.random.choice(entity_list, size=n)\n",
    "\n",
    "        corrupted_heads = edge_list.detach().clone()\n",
    "        corrupted_tails = edge_list.detach().clone()\n",
    "\n",
    "        corrupted_heads[:,0] = torch.from_numpy(r1)\n",
    "        corrupted_tails[:,1] = torch.from_numpy(r2)\n",
    "                \n",
    "        return corrupted_heads, corrupted_tails\n",
    "\n",
    "    def corrupt_edge_list(self, edge_list: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" given a list of triplets return a single list of triplet where either\n",
    "            the head or the tail has been randomly replaced, but not both \"\"\"\n",
    "        n = edge_list.shape[0]\n",
    "        entity_list = range(self.num_entities)\n",
    "\n",
    "        heads = edge_list[:,0]\n",
    "        tails = edge_list[:, 1]\n",
    "\n",
    "        # sample random entity replacements\n",
    "        sample = np.random.choice(entity_list, size=n)\n",
    "        sample = torch.from_numpy(sample).type(torch.int64).to(self.device)\n",
    "\n",
    "        # random selection of either head or tail\n",
    "        pos = np.random.choice([0, 1], size=n)\n",
    "        pos = torch.from_numpy(pos).type(torch.int64).to(self.device)\n",
    "        pos = pos.reshape(-1,1)\n",
    "\n",
    "        # create a tensor of two columns where the first is head/tail\n",
    "        # and the second represents the random sample of entities\n",
    "        corrupted_heads = torch.vstack([heads, sample]).T\n",
    "        corrupted_tails = torch.vstack([tails, sample]).T\n",
    "\n",
    "        # keep either the head/tail or the random entity from each tensor\n",
    "        corrupted_heads = corrupted_heads.gather(1, pos.reshape(-1,1))\n",
    "        corrupted_tails = corrupted_tails.gather(1, (1-pos).reshape(-1,1))\n",
    "\n",
    "        # combine the resulting triplet with the gurantee that either head or\n",
    "        # tail has been randomly replaced but not both at the same time\n",
    "        corrupted_triplet = torch.hstack([corrupted_heads, corrupted_tails])\n",
    "        \n",
    "        return corrupted_triplet\n",
    "\n",
    "    \n",
    "    def embedding_loss(self, batch: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" returns the margin ranking loss for a batch of triplets\n",
    "            according to the corruption procedure defined at page 3 \"\"\"\n",
    "        edge_list, labels = batch\n",
    "        \n",
    "        loss = torch.zeros(1).to(self.device)\n",
    "\n",
    "        # to use double corruption uncomment the following line\n",
    "        #corrupted_heads, corrupted_tails = self.corrupt_edge_list(edge_list)\n",
    "        corrupted_triplet = self.corrupt_edge_list(edge_list)\n",
    "        \n",
    "        # to use the double list corruption uncommented the following\n",
    "        # lines and comment the alternative definitions\n",
    "        \"\"\" t1 = self.entity_mat.weight[edge_list.repeat(2,1)]          \n",
    "        t2 = torch.vstack([self.entity_mat.weight[corrupted_heads],    \n",
    "                           self.entity_mat.weight[corrupted_tails]])\n",
    "        rel = self.relation_mat.weight[labels].repeat(2,1) \"\"\"\n",
    "\n",
    "        # (single) corruption procedure\n",
    "        t1 = self.entity_mat.weight[edge_list]\n",
    "        t2 = self.entity_mat.weight[corrupted_triplet]\n",
    "        rel = self.relation_mat.weight[labels]\n",
    "\n",
    "        # normalize entity (maybe unnecessary here)\n",
    "        t1 = F.normalize(t1, p=self.p_norm, dim=-1)\n",
    "        t2 = F.normalize(t2, p=self.p_norm, dim=-1)\n",
    "\n",
    "        # margin ranking loss, dim1 represents a triplet, dim2 represent either head=0\n",
    "        # or tail=1 and dim3 is the embedding representation of the entity\n",
    "        pos = torch.norm(t1[:,0,:] + rel - t1[:,1,:], dim=-1, p=self.p_norm)\n",
    "        neg = torch.norm(t2[:,0,:] + rel - t2[:,1,:], dim=-1, p=self.p_norm)\n",
    "        loss = torch.clip((self.margin + pos - neg), min=0).sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluation_protocol(self, batch: torch.Tensor):\n",
    "        edge_list, labels = batch\n",
    "        batch_size = edge_list.shape[0]\n",
    "\n",
    "        # combine heads, tails and labels\n",
    "        triplets = torch.hstack([edge_list, labels.reshape(-1,1)])\n",
    "\n",
    "        # repeat all triplets for n_entities times\n",
    "        triplets = triplets[:,np.newaxis,:].repeat(1,self.num_entities,1)\n",
    "\n",
    "        true_pos_total = list()\n",
    "        rank_pos_list = list()\n",
    "\n",
    "        # repeat corruption for both head and tail\n",
    "        for pos in [0,1]:\n",
    "            x = triplets.detach().clone()\n",
    "            \n",
    "            # replace all heads/tails with list of all possible entities\n",
    "            x[:,:,pos] = torch.tensor(range(self.num_entities))[np.newaxis,:].repeat(batch_size,1).to(self.device)\n",
    "\n",
    "            # triplets are arranged as (head, tail, label)\n",
    "            head = self.entity_mat.weight[x[:,:,0]]\n",
    "            tail = self.entity_mat.weight[x[:,:,1]]\n",
    "            rel = self.relation_mat.weight[x[:,:,2]]\n",
    "\n",
    "            # compute distance between head + label and tail\n",
    "            norms = torch.norm(head + rel - tail, dim=-1, p=self.p_norm)\n",
    "\n",
    "            # get index positions of sorted norms for each triplet\n",
    "            rankings = torch.vstack([torch.argsort(x) for x in norms.unbind(dim=0)])\n",
    "\n",
    "            # find position of heads within the rankings\n",
    "            torch.save(rankings, \"rankings.pt\")\n",
    "            torch.save(edge_list, \"edge_list.pt\")\n",
    "            rank_pos = torch.where(rankings == edge_list[:,pos].reshape(-1,1))[1]\n",
    "\n",
    "            rank_pos_list.append(rank_pos)\n",
    "            true_pos_total.append(rank_pos < 10)\n",
    "\n",
    "        return torch.vstack(rank_pos_list).flatten()\n",
    "\n",
    "    def training_step(self, batch: torch.Tensor, batch_idx: int):\n",
    "        loss_dict = {'train_loss': self.embedding_loss(batch)}\n",
    "        self.log_dict(loss_dict, logger=True)\n",
    "\n",
    "    def validation_step(self, batch: torch.Tensor, batch_idx: int):\n",
    "        loss = self.embedding_loss(batch)\n",
    "        batch_rankings = self.evaluation_protocol(batch)\n",
    "        self.log_dict({\"val_loss\": loss}, prog_bar=True, on_epoch=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"batch_rankings\": batch_rankings}\n",
    "    \n",
    "    def predict_step(self, batch: torch.Tensor, batch_idx: int):\n",
    "        batch_rankings = self.evaluation_protocol(batch)\n",
    "        return {\"batch_rankings\": batch_rankings}\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        with torch.no_grad():\n",
    "             # keep entities embeddings normalized\n",
    "            self.entity_mat.weight.copy_(F.normalize(self.entity_mat.weight, p=2, dim=1))\n",
    "    \n",
    "    def compute_epoch_metrics(self, outputs: List[torch.Tensor], stage: str, log_value: bool=True):\n",
    "        \"\"\" compute loss and metrics for the current epoch outputs \"\"\"\n",
    "        epoch_rankings = torch.hstack([x['batch_rankings'] for x in outputs])\n",
    "        mean_rank = epoch_rankings.float().float().mean()\n",
    "        hit_at_10 = (epoch_rankings < 10).float().mean()*100\n",
    "        if log_value:\n",
    "            self.log_dict({f\"{stage}_mean_rank\": mean_rank,\n",
    "                           f\"{stage}_hits@10\": hit_at_10},\n",
    "                           prog_bar=True, on_epoch=True, logger=True)\n",
    "        else:\n",
    "            return mean_rank, hit_at_10\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.compute_epoch_metrics(outputs, stage=\"val\")\n",
    "\n",
    "    def prediction_epoch_end(self, outputs):\n",
    "        self.compute_epoch_metrics(outputs, stage=\"predict\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transe(config: dict,\n",
    "                 max_epochs: int=100,\n",
    "                 accelerator: str='gpu',\n",
    "                 num_best_ckpt: int=3,\n",
    "                 patience: int=10,\n",
    "                 min_delta: float=0.5,\n",
    "                 main_path: str='./',\n",
    "                 dataset: str='WordNet18RR') -> None:\n",
    "    \"\"\" train a TransE model on the input parameters\n",
    "\n",
    "    Args:\n",
    "        config (dict): dictionary containing keys [emb_dim, lr, margin, p_norm]\n",
    "        max_epochs (int, optional): maximum number of epochs. Defaults to 100.\n",
    "        accelerator (str, optional): pytorch accelerator. Defaults to 'gpu'.\n",
    "        num_best_ckpt (int, optional): number of best models to save through training. Defaults to 3.\n",
    "        patience (int, optional): number of epochs to wait for early stopping. Defaults to 10.\n",
    "        min_delta (float, optional): minimum change of meank rank for early stopping. Defaults to 0.5.\n",
    "        main_path (str, optional): main path to store models. Defaults to './'.\n",
    "        dataset (str, optional): dataset to train on.  Defaults to 'WordNet18RR'.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = TransE(emb_dim=config['emb_dim'],\n",
    "                learning_rate=config['lr'],\n",
    "                margin=config['margin'],\n",
    "                p_norm=config['p_norm'],\n",
    "                dataset=dataset)\n",
    "\n",
    "    dm = WordNetDataModule(batch_size=config['batch_size'], dataset=dataset)\n",
    "\n",
    "    dir_path = f\"ckpt_{dataset}/emb_dim={config['emb_dim']}-lr={config['lr']}-margin={config['margin']}-p_norm={config['p_norm']}\"\n",
    "    dir_path = os.path.join(main_path, dir_path)\n",
    "\n",
    "    # using mean predicted rank on validation set as described in section 4.2\n",
    "    early_stop_rank = EarlyStopping(monitor=\"val_mean_rank\",\n",
    "                                    min_delta=min_delta,\n",
    "                                    patience=patience,\n",
    "                                    verbose=False,\n",
    "                                    mode=\"min\")\n",
    "\n",
    "    # save best models based on mean rank on validation set\n",
    "    checkpoint_callback = ModelCheckpoint(save_top_k=num_best_ckpt,\n",
    "                                        monitor=\"val_mean_rank\",\n",
    "                                        dirpath=dir_path,\n",
    "                                        filename=\"transe-{dataset}-{epoch}-{val_mean_rank:.0f}-{val_hits@10:.1f}\")\n",
    "\n",
    "    # loggin using TensorBoard\n",
    "    logger = TensorBoardLogger(f'tb_logs_{dataset}', name='TransE')\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs,\n",
    "                        accelerator=accelerator,\n",
    "                        callbacks=[checkpoint_callback, early_stop_rank],\n",
    "                        logger=logger)\n",
    "\n",
    "    try:\n",
    "        # resume from best model if checkpoint is available\n",
    "        ckpt_path = os.path.join(dir_path, os.listdir(dir_path)[-1])\n",
    "    except:\n",
    "        ckpt_path = None\n",
    "\n",
    "    trainer.fit(model, datamodule=dm, ckpt_path=ckpt_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters search\n",
    "\n",
    "The following cells show how to train the model on a range of parameters or on some specific target parameters. A few notes here:\n",
    "\n",
    "- we trained the model using the same parameter configuration from Bordes et al. but on the number of dimension since 50 didn't fit in memory with a batch size of 128. For efficiency purposes we opted to keep a higher batch size and lower emb dim, though a few test have been done with batch_size=16 and emb_dim=50 which didn't beat the best model we found on the suggested configuration.\n",
    "- given underwhelming performance on the WordNet18RR (~40% hits@10) compared to the numbers achieved from the authors on the original WordNet18 (~75% hits@10) we decided to test the model also on the latter but we were not able to replicate the same results only reaching ~60% hits@10, see later for details.\n",
    "- different attempts have been made in order to figure out why the difference in performance, such as using a different normalization procedure, different parameters for early stopping, using a from scratch and an out of library maring ranking loss, etc. none of those resulted in a substantial improvement\n",
    "\n",
    "All models are available under the ckpt_{dataset} directory where the filename highlight number of epochs, mean rank and hits@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model: emb_dim=40, lr=0.001, margin=1, p_norm=2\n",
    "\n",
    "# add values to each list to enlarge parameter search\n",
    "# all models get saved into 'ckpt_{dataset}'\n",
    "\n",
    "# embd_dim=50 was used in the paper but on the current setup it requires a smaller\n",
    "# batch size and hence a much slower training time, which is why 40 was used instead\n",
    "config = {\n",
    "    \"batch_size\": [128],\n",
    "    \"lr\": [0.001, 0.01, 0.1],\n",
    "    \"emb_dim\": [20, 40],\n",
    "    \"p_norm\": [1],\n",
    "    \"margin\": [1, 2]\n",
    "}\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "# a from scratch method for parameter search was used due to some printing\n",
    "# issues with tqdm that resulted in a messy output\n",
    "keys, values = zip(*config.items())\n",
    "comb_list = [dict(zip(keys,v)) for v in itertools.product(*values)]\n",
    "\n",
    "for comb in comb_list:\n",
    "    with suppress(Exception):\n",
    "        train_transe(config=comb, max_epochs=num_epochs, dataset=\"WordNet18RR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type      | Params\n",
      "-------------------------------------------\n",
      "0 | entity_mat   | Embedding | 1.6 M \n",
      "1 | relation_mat | Embedding | 440   \n",
      "-------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.553     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  94%|█████████▎| 659/703 [00:11<00:00, 56.97it/s, loss=74.8, v_num=69, val_loss=100.0, val_mean_rank=1.08e+4, val_hits@10=8.600]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# use this cell to run a single training instance\n",
    "single_train_config = {\n",
    "    'batch_size': 128,\n",
    "    'lr': 0.001,\n",
    "    'emb_dim': 40,\n",
    "    'p_norm': 2,\n",
    "    'margin': 1\n",
    "}\n",
    "\n",
    "train_transe(config=single_train_config, main_path=\"./ckpt_WordNet18RR/\", dataset=\"WordNet18RR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on WordNet18RR (and WN18)\n",
    "The following cell defines a prediction function that takes the configuration of best model, loads the single best among the topk (=3) stored earlier during training and prints out the result. The best performance achieved on the test set of *WordNet18RR* has been *hits@10 of 41.6%* and a *mean rank of ~2975*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at ./ckpt_WordNet18RR/emb_dim=40-lr=0.001-margin=1-p_norm=2/transe-wordnet-epoch=46-val_mean_rank=2854-val_hits@10=41.6.ckpt\n",
      "Loaded model weights from checkpoint at ./ckpt_WordNet18RR/emb_dim=40-lr=0.001-margin=1-p_norm=2/transe-wordnet-epoch=46-val_mean_rank=2854-val_hits@10=41.6.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 25/25 [01:33<00:00,  3.75s/it]\n",
      "\n",
      "\n",
      "test_mean_rank=2975, test_hits@10=41.59%\n"
     ]
    }
   ],
   "source": [
    "def hit10_from_filename(filename):\n",
    "    filename = filename.replace('.ckpt','')\n",
    "    return float(filename.split('@10=')[1])\n",
    "\n",
    "def predict_transe(config: dict):\n",
    "    \"\"\" outputs the mean rank and hist@10 for the input configuration \"\"\"\n",
    "    trainer = pl.Trainer()\n",
    "\n",
    "    dm = WordNetDataModule(batch_size=config['batch_size'], dataset=config['dataset'])\n",
    "\n",
    "    # ckpt_path from model config\n",
    "    path = f\"./ckpt_{config['dataset']}/emb_dim={config['emb_dim']}-lr={config['learning_rate']}-margin={config['margin']}-p_norm={config['p_norm']}/\"\n",
    "\n",
    "    # take filename of model with highest hit@10\n",
    "    filename = max(os.listdir(path), key=hit10_from_filename)\n",
    "    \n",
    "    model = TransE(emb_dim=config['emb_dim'],\n",
    "                learning_rate=config['learning_rate'],\n",
    "                margin=config['margin'],\n",
    "                p_norm=config['p_norm'],\n",
    "                dataset=config['dataset'])\n",
    "    \n",
    "    pred = trainer.predict(model, datamodule=dm, ckpt_path=os.path.join(path, filename))\n",
    "    test_mean_rank, test_hits_at_10 = model.compute_epoch_metrics(pred, stage=\"predict\", log_value=False)\n",
    "    print(\"\\n\")\n",
    "    print(f\"test_mean_rank={test_mean_rank:.0f}, test_hits@10={test_hits_at_10:.2f}%\")\n",
    "\n",
    "# PREDICTION ON TEST SET\n",
    "\n",
    "# set the following config to match the best model parameters\n",
    "best_model_config = {\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 0.001,\n",
    "    'emb_dim': 40,\n",
    "    'p_norm': 2,\n",
    "    'margin': 1,\n",
    "    'dataset': 'WordNet18RR'\n",
    "}\n",
    "\n",
    "predict_transe(config=best_model_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness we report the results on WordNet18 with the best model configuration (the same achieved on WN18RR):\n",
    "- hits@10 = 61.7%\n",
    "- mean rank = 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  12%|█▎        | 5/40 [11:24<1:19:53, 136.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ./ckpt_WordNet18/emb_dim=40-lr=0.001-margin=1-p_norm=2/transe-dataset=0-epoch=88-val_mean_rank=202-val_hits@10=61.2.ckpt\n",
      "Loaded model weights from checkpoint at ./ckpt_WordNet18/emb_dim=40-lr=0.001-margin=1-p_norm=2/transe-dataset=0-epoch=88-val_mean_rank=202-val_hits@10=61.2.ckpt\n",
      "C:\\Users\\Uni\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 40/40 [02:55<00:00,  4.38s/it]\n",
      "\n",
      "\n",
      "test_mean_rank=170, test_hits@10=61.71%\n"
     ]
    }
   ],
   "source": [
    "# for sake of completeness we report the test set performance on WordNet18\n",
    "best_model_config = {\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 0.001,\n",
    "    'emb_dim': 40,\n",
    "    'p_norm': 2,\n",
    "    'margin': 1,\n",
    "    'dataset': 'WordNet18'\n",
    "}\n",
    "\n",
    "predict_transe(config=best_model_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
