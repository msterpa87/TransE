{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write implementation on PyTorch for TransE model (you can use TorchGeometric or DGL library for working with graphs) and train your model on WordNet18RR dataset (you can use loaded dataset from any graph library).\n",
    "\n",
    "As a result, you must provide a link to github (or gitlab) with all the source code.\n",
    "The readability of the code, the presence of comments, type annotations, and the quality of the code as a whole will be taken into account when checking the test case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union, Callable, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import WordNet18RR\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download wordnet dataset, we'll be using the processed file data.pt\n",
    "dataset = WordNet18RR('./WordNet18RR/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge():\n",
    "    def __init__(self, head, tail, rel) -> None:\n",
    "        self.head = head\n",
    "        self.tail = tail\n",
    "        self.rel = rel\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.head} {self.rel} {self.tail}\"\n",
    "\n",
    "def process_lines(lines):\n",
    "    return list(map(lambda s: s.strip('\\n').split('\\t'), lines))\n",
    "\n",
    "def load_edges_from_file(path: str, header: bool=False):\n",
    "    edge_list = list()\n",
    "\n",
    "    lines = open(path).readlines()\n",
    "    lines = process_lines(lines)\n",
    "\n",
    "    edge_list = [Edge(head=head, tail=tail, rel=rel) for head, rel, tail in lines]\n",
    "    \n",
    "    return edge_list\n",
    "\n",
    "def load_ids_dict(path):\n",
    "    entity2id = process_lines(open(os.path.join(WORDNET_PATH, \"entity2id.txt\")))\n",
    "    relation2id = process_lines(open(os.path.join(WORDNET_PATH, \"relation2id.txt\")))\n",
    "\n",
    "    entity2id = dict([(x[0], int(x[1])) for x in entity2id])\n",
    "    relation2id = dict([(x[0], int(x[1])) for x in relation2id])\n",
    "\n",
    "    return entity2id, relation2id\n",
    "\n",
    "# create entity2id and relation2id files\n",
    "WORDNET_PATH = \"WordNet18RR/raw/\"\n",
    "\n",
    "train_edge_list = load_edges_from_file(os.path.join(WORDNET_PATH, \"train.txt\"))\n",
    "val_edge_list = load_edges_from_file(os.path.join(WORDNET_PATH, \"valid.txt\"))\n",
    "test_edge_list = load_edges_from_file(os.path.join(WORDNET_PATH, \"test.txt\"))\n",
    "\n",
    "entity_list = list()\n",
    "relation_list = list()\n",
    "\n",
    "for edge_list in [train_edge_list, val_edge_list, test_edge_list]:\n",
    "    entity_list += [x.head for x in edge_list] + [x.tail for x in edge_list]\n",
    "    relation_list += [x.rel for x in edge_list]\n",
    "\n",
    "entity_list = sorted(list(set(entity_list)))\n",
    "entity2id = dict(zip(entity_list, range(len(entity_list))))\n",
    "\n",
    "relation_list = sorted(list(set(relation_list)))\n",
    "relation2id = dict(zip(relation_list, range(len(relation_list))))\n",
    "\n",
    "with open(\"WordNet18RR/raw/entity2id.txt\", \"w\") as f:\n",
    "    f.writelines([f\"{x}\\t{y}\\n\" for x,y in entity2id.items()])\n",
    "\n",
    "with open(\"WordNet18RR/raw/relation2id.txt\", \"w\") as f:\n",
    "    f.writelines([f\"{x}\\t{y}\\n\" for x,y in relation2id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordNetDataset(Dataset):\n",
    "    def __init__(self, path: str=\"WordNet18RR/raw/\", split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.split = split\n",
    "\n",
    "        edge_list = load_edges_from_file(os.path.join(self.path, f\"{self.split}.txt\"))\n",
    "        entity2id, relation2id = load_ids_dict(path)\n",
    "\n",
    "        self.edge_list = torch.tensor([(entity2id[e.head], entity2id[e.tail]) for e in edge_list])\n",
    "        self.relation_list = torch.tensor([relation2id[e.rel] for e in edge_list])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.edge_list.shape[0]\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[int,int]:\n",
    "        return self.edge_list[index], self.relation_list[index]\n",
    "\n",
    "class WordNetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, path: str=\"WordNet18RR/raw/\", batch_size=32) -> None:\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_entities = 40943\n",
    "        self.num_relations = 11\n",
    "        self.params = {\"pin_memory\": True, \"batch_size\": batch_size}\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = WordNetDataset(path=self.path, split=\"train\")\n",
    "            self.val_dataset = WordNetDataset(path=self.path, split=\"valid\")\n",
    "        \n",
    "        if stage == \"predict\":\n",
    "            self.test_dataset = WordNetDataset(path=self.path, split=\"test\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, shuffle=True, **self.params)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, shuffle=False, **self.params)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, shuffle=False, **self.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37606, 37484), 3)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = WordNetDataset()\n",
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dm \u001b[39m=\u001b[39m WordNetDataModule()\n\u001b[0;32m      2\u001b[0m dm\u001b[39m.\u001b[39msetup(\u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dm\u001b[39m.\u001b[39;49mtrain_dataloader()))\n",
      "Cell \u001b[1;32mIn[102], line 37\u001b[0m, in \u001b[0;36mWordNetDataModule.train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_dataloader\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m DataLoader(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataset, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning_fabric\\utilities\\data.py:323\u001b[0m, in \u001b[0;36m_wrap_init_method.<locals>.wrapper\u001b[1;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39melif\u001b[39;00m store_explicit_arg \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    321\u001b[0m         \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m{\u001b[39;00mstore_explicit_arg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, kwargs[store_explicit_arg])\n\u001b[1;32m--> 323\u001b[0m init(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    324\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__pl_inside_init\u001b[39m\u001b[39m\"\u001b[39m, old_inside_init)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning_fabric\\utilities\\data.py:323\u001b[0m, in \u001b[0;36m_wrap_init_method.<locals>.wrapper\u001b[1;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39melif\u001b[39;00m store_explicit_arg \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    321\u001b[0m         \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m{\u001b[39;00mstore_explicit_arg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, kwargs[store_explicit_arg])\n\u001b[1;32m--> 323\u001b[0m init(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    324\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__pl_inside_init\u001b[39m\u001b[39m\"\u001b[39m, old_inside_init)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning_fabric\\utilities\\data.py:323\u001b[0m, in \u001b[0;36m_wrap_init_method.<locals>.wrapper\u001b[1;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39melif\u001b[39;00m store_explicit_arg \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    321\u001b[0m         \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m{\u001b[39;00mstore_explicit_arg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, kwargs[store_explicit_arg])\n\u001b[1;32m--> 323\u001b[0m init(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    324\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__pl_inside_init\u001b[39m\u001b[39m\"\u001b[39m, old_inside_init)\n",
      "File \u001b[1;32mc:\\Users\\Uni\\miniconda3\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:78\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, follow_batch, exclude_keys, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_batch \u001b[39m=\u001b[39m follow_batch\n\u001b[0;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexclude_keys \u001b[39m=\u001b[39m exclude_keys\n\u001b[1;32m---> 78\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m     79\u001b[0m     dataset,\n\u001b[0;32m     80\u001b[0m     batch_size,\n\u001b[0;32m     81\u001b[0m     shuffle,\n\u001b[0;32m     82\u001b[0m     collate_fn\u001b[39m=\u001b[39mCollater(follow_batch, exclude_keys),\n\u001b[0;32m     83\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     84\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning_fabric\\utilities\\data.py:323\u001b[0m, in \u001b[0;36m_wrap_init_method.<locals>.wrapper\u001b[1;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39melif\u001b[39;00m store_explicit_arg \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    321\u001b[0m         \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m{\u001b[39;00mstore_explicit_arg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, kwargs[store_explicit_arg])\n\u001b[1;32m--> 323\u001b[0m init(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    324\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__pl_inside_init\u001b[39m\u001b[39m\"\u001b[39m, old_inside_init)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning_fabric\\utilities\\data.py:323\u001b[0m, in \u001b[0;36m_wrap_init_method.<locals>.wrapper\u001b[1;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39melif\u001b[39;00m store_explicit_arg \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    321\u001b[0m         \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m{\u001b[39;00mstore_explicit_arg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, kwargs[store_explicit_arg])\n\u001b[1;32m--> 323\u001b[0m init(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    324\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__pl_inside_init\u001b[39m\u001b[39m\"\u001b[39m, old_inside_init)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning_fabric\\utilities\\data.py:323\u001b[0m, in \u001b[0;36m_wrap_init_method.<locals>.wrapper\u001b[1;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39melif\u001b[39;00m store_explicit_arg \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    321\u001b[0m         \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m{\u001b[39;00mstore_explicit_arg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, kwargs[store_explicit_arg])\n\u001b[1;32m--> 323\u001b[0m init(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    324\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__pl_inside_init\u001b[39m\u001b[39m\"\u001b[39m, old_inside_init)\n",
      "File \u001b[1;32mc:\\Users\\Uni\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:344\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# map-style\u001b[39;00m\n\u001b[0;32m    343\u001b[0m     \u001b[39mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 344\u001b[0m         sampler \u001b[39m=\u001b[39m RandomSampler(dataset, generator\u001b[39m=\u001b[39;49mgenerator)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    345\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m         sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Uni\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\sampler.py:106\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement, \u001b[39mbool\u001b[39m):\n\u001b[0;32m    103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreplacement should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mreplacement=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement))\n\u001b[1;32m--> 106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_samples, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_samples should be a positive integer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mvalue, but got num_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples))\n",
      "File \u001b[1;32mc:\\Users\\Uni\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\sampler.py:114\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnum_samples\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m    112\u001b[0m     \u001b[39m# dataset size might change at runtime\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_samples \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_source)\n\u001b[0;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_samples\n",
      "Cell \u001b[1;32mIn[102], line 14\u001b[0m, in \u001b[0;36mWordNetDataset.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_list\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "dm = WordNetDataModule()\n",
    "dm.setup(\"fit\")\n",
    "next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(pl.LightningModule):\n",
    "    def __init__(self, margin: int=1, emb_dim: int=50, learning_rate=0.01, p_norm=1) -> None:\n",
    "        \"\"\" Instatiate the entity and relation matrix of the TransE model\n",
    "            https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data\n",
    "\n",
    "        Args:\n",
    "            n_entities (int): _description_\n",
    "            n_relations (int): _description_\n",
    "            margin (int, optional): _description_. Defaults to 1.\n",
    "            emb_dim (int, optional): _description_. Defaults to 50.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.emb_dim = emb_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.p_norm = p_norm\n",
    "\n",
    "        # dataset specific values\n",
    "        self.num_entities = 40943\n",
    "        self.num_relations = 11\n",
    "\n",
    "        # initialize embeddings\n",
    "        self.entity_mat = nn.Embedding(self.num_entities, emb_dim).to(self.device)\n",
    "        self.relation_mat = nn.Embedding(self.num_relations, emb_dim).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # initialize with random uniform\n",
    "            val = 6/np.sqrt(emb_dim)\n",
    "            self.entity_mat.weight.uniform_(-val, val)\n",
    "            self.relation_mat.weight.uniform_(-val, val)\n",
    "\n",
    "            # normalize entity and relation embeddings\n",
    "            self.entity_mat.weight.copy_(F.normalize(self.entity_mat.weight, p=self.p_norm, dim=-1))\n",
    "            self.relation_mat.weight.copy_(F.normalize(self.relation_mat.weight, p=self.p_norm, dim=-1))\n",
    "\n",
    "    def corrupt_edge_list(self, edge_list: torch.Tensor):\n",
    "        \"\"\" sample either the head or tail of x from range(n) \"\"\"\n",
    "        n = edge_list.shape[0]\n",
    "        entity_list = range(self.num_entities)\n",
    "\n",
    "        # sample random entity replacements\n",
    "        r1 = np.random.choice(entity_list, size=n)\n",
    "        r2 = np.random.choice(entity_list, size=n)\n",
    "\n",
    "        corrupted_heads = edge_list.detach().clone()\n",
    "        corrupted_tails = edge_list.detach().clone()\n",
    "\n",
    "        corrupted_heads[:,0] = torch.from_numpy(r1)\n",
    "        corrupted_tails[:,1] = torch.from_numpy(r2)\n",
    "                \n",
    "        return corrupted_heads, corrupted_tails\n",
    "    \n",
    "    def embedding_loss(self, batch):\n",
    "        edge_list, labels = batch\n",
    "        \n",
    "        loss = torch.zeros(1).to(self.device)\n",
    "\n",
    "        #edge_list_cor = self.corrupt_edge_list(edge_list)\n",
    "        corrupted_heads, corrupted_tails = self.corrupt_edge_list(edge_list)\n",
    "        \n",
    "        # take embedding values for entities and relations\n",
    "        t1 = self.entity_mat.weight[edge_list.repeat(2,1)]\n",
    "        t2 = torch.vstack([self.entity_mat.weight[corrupted_heads],\n",
    "                           self.entity_mat.weight[corrupted_tails]])\n",
    "        rel = self.relation_mat.weight[labels].repeat(2,1)\n",
    "\n",
    "        # normalize entity (maybe unnecessary here)\n",
    "        t1 = F.normalize(t1, p=self.p_norm, dim=-1)\n",
    "        t2 = F.normalize(t2, p=self.p_norm, dim=-1)\n",
    "\n",
    "        # compute the loss value\n",
    "        pos = torch.norm(t1[:,0,:] + rel - t1[:,1,:], dim=-1, p=self.p_norm)\n",
    "        neg = torch.norm(t2[:,0,:] + rel - t2[:,1,:], dim=-1, p=self.p_norm)\n",
    "        loss = torch.clip((self.margin + pos - neg), min=0).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluation_protocol(self, batch):\n",
    "        edge_list, labels = batch\n",
    "        batch_size = edge_list.shape[0]\n",
    "\n",
    "        # combine heads, tails and labels\n",
    "        triplets = torch.hstack([edge_list, labels.reshape(-1,1)])\n",
    "\n",
    "        # repeat all triplets for n_entities times\n",
    "        triplets = triplets[:,np.newaxis,:].repeat(1,self.num_entities,1)\n",
    "\n",
    "        true_pos_total = list()\n",
    "        rank_pos_list = list()\n",
    "\n",
    "        # repeat corruption for both head and tail\n",
    "        for pos in [0,1]:\n",
    "            x = triplets.detach().clone()\n",
    "            \n",
    "            # replace all heads/tails with list of all possible entities\n",
    "            x[:,:,pos] = torch.tensor(range(self.num_entities))[np.newaxis,:].repeat(batch_size,1).to(self.device)\n",
    "\n",
    "            # triplets are arranged as (head, tail, label)\n",
    "            head = self.entity_mat.weight[x[:,:,0]]\n",
    "            tail = self.entity_mat.weight[x[:,:,1]]\n",
    "            rel = self.relation_mat.weight[x[:,:,2]]\n",
    "\n",
    "            # compute distance between head + label and tail\n",
    "            norms = torch.norm(head + rel - tail, dim=-1, p=self.p_norm)\n",
    "\n",
    "            # get index positions of sorted norms for each triplet\n",
    "            rankings = torch.vstack([torch.argsort(x) for x in norms.unbind(dim=0)])\n",
    "\n",
    "            # find position of heads within the rankings\n",
    "            rank_pos = torch.where(rankings == edge_list[:,pos].reshape(-1,1))[1]\n",
    "\n",
    "            rank_pos_list.append(rank_pos)\n",
    "            true_pos_total.append(rank_pos < 10)\n",
    "\n",
    "        mean_rank = torch.vstack(rank_pos_list).float().mean()\n",
    "        hits_at_10 = torch.vstack(true_pos_total).float().mean()*100\n",
    "\n",
    "        return mean_rank, hits_at_10\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.embedding_loss(batch)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.embedding_loss(batch)\n",
    "        mean_rank, hits_at_10 = self.evaluation_protocol(batch)\n",
    "        metrics = {\"val_loss\": loss, \"val_mean_rank\": mean_rank, \"val_hits@10\": hits_at_10}\n",
    "        self.log_dict(metrics, prog_bar=True, on_epoch=True)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.embedding_loss(batch)\n",
    "        mean_rank, hits_at_10 = self.evaluation_protocol(batch)\n",
    "        metrics = {\"test_loss\": loss, \"test_mean_rank\": mean_rank, \"test_hits@10\": hits_at_10}\n",
    "        self.log_dict(metrics, prog_bar=True, on_epoch=True)\n",
    "        return metrics\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        mean_rank, hits_at_10 = self.evaluation_protocol(batch)\n",
    "        return mean_rank, hits_at_10\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        with torch.no_grad():\n",
    "             # keep entities embeddings normalized\n",
    "            self.entity_mat.weight.copy_(F.normalize(self.entity_mat.weight, p=2, dim=1))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at checkpoints/emb_dim=20-lr=0.01-margin=1-p_norm=2\\transe-wordnet-epoch=0-val_mean_rank=20402-val_hits@10=0.0.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type      | Params\n",
      "-------------------------------------------\n",
      "0 | entity_mat   | Embedding | 818 K \n",
      "1 | relation_mat | Embedding | 220   \n",
      "-------------------------------------------\n",
      "819 K     Trainable params\n",
      "0         Non-trainable params\n",
      "819 K     Total params\n",
      "3.276     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at checkpoints/emb_dim=20-lr=0.01-margin=1-p_norm=2\\transe-wordnet-epoch=0-val_mean_rank=20402-val_hits@10=0.0.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 16/2809 [00:00<00:34, 81.85it/s, loss=0.992, v_num=25]"
     ]
    }
   ],
   "source": [
    "emb_dim = 20\n",
    "lr = 0.01\n",
    "margin = 1\n",
    "max_epochs = 1000\n",
    "top_k_cp = 3\n",
    "p_norm = 2   # norm either L1 or L2\n",
    "\n",
    "# instantiated model and data module\n",
    "model = TransE(emb_dim=emb_dim,\n",
    "               learning_rate=lr,\n",
    "               margin=margin,\n",
    "               p_norm=p_norm)\n",
    "\n",
    "dm = WordNetRawDataModule(batch_size=32)\n",
    "\n",
    "dir_path = f\"checkpoints/emb_dim={emb_dim}-lr={lr}-margin={margin}-p_norm={p_norm}\"\n",
    "\n",
    "# using mean predicted rank on validation set as described in section 4.2\n",
    "early_stop_rank = EarlyStopping(monitor=\"val_mean_rank\",\n",
    "                                min_delta=0.5,\n",
    "                                patience=10,\n",
    "                                verbose=False,\n",
    "                                mode=\"min\")\n",
    "\n",
    "# save best models based on mean rank on validation set\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=top_k_cp,\n",
    "                                      monitor=\"val_mean_rank\",\n",
    "                                      dirpath=dir_path,\n",
    "                                      filename=\"transe-wordnet-{epoch}-{val_mean_rank:.0f}-{val_hits@10:.1f}\")\n",
    "\n",
    "logger = TensorBoardLogger('tb_logs', name='TransE')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=max_epochs,\n",
    "                     accelerator='gpu',\n",
    "                     callbacks=[checkpoint_callback,early_stop_rank],\n",
    "                     logger=logger)\n",
    "\n",
    "try:\n",
    "    # resume from best model if checkpoint is available\n",
    "    ckpt_path = os.path.join(dir_path, os.listdir(dir_path)[-1])\n",
    "except:\n",
    "    ckpt_path = None\n",
    "\n",
    "trainer.fit(model, datamodule=dm, ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at checkpoints/emb_dim=20-lr=0.01-margin=1-p_norm=2\\transe-wordnet-epoch=0-val_mean_rank=20402-val_hits@10=0.0.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at checkpoints/emb_dim=20-lr=0.01-margin=1-p_norm=2\\transe-wordnet-epoch=0-val_mean_rank=20402-val_hits@10=0.0.ckpt\n",
      "C:\\Users\\Uni\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 98/98 [00:06<00:00, 14.84it/s]\n",
      "test_mean_rank=20634, test_hits@10=0.03\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(model, datamodule=dm, ckpt_path=ckpt_path)\n",
    "test_mean_rank, test_hits_at_10 = torch.tensor(pred).mean(0)\n",
    "print(\"\\n\")\n",
    "print(f\"test_mean_rank={test_mean_rank:.0f}, test_hits@10={test_hits_at_10:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
